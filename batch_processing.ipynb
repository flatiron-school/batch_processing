{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1982b62",
   "metadata": {},
   "source": [
    "# Batch Processing with `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20e419fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import random\n",
    "import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Callable, Dict, List, Union\n",
    "from multiprocessing import Queue\n",
    "\n",
    "import tqdm as tqdm_\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm_batch import batch_process\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba531e08",
   "metadata": {},
   "source": [
    "![img](https://res.cloudinary.com/hevo/images/f_auto,q_auto/v1649315584/hevo-learn/Batch-Processing-Batch-Processing-vs-Stream-Processing/Batch-Processing-Batch-Processing-vs-Stream-Processing.png?_i=AA)\n",
    "\n",
    "(Source: https://hevodata.com/learn/batch-processing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65631636",
   "metadata": {},
   "source": [
    "Batch processing is to be contrasted with serial or *stream* processing. Stream processing is critical when you need real-time updating of data reports or analyses. But if you are processing large chunks of data, it can be better to process it in batches.\n",
    "\n",
    "Batch processing works in an **automated** way based on a **scheduler**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d5827",
   "metadata": {},
   "source": [
    "## Some Advantages of `joblib`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbdbd6",
   "metadata": {},
   "source": [
    "- Disk Caching of Functions & Lazy Re-Evaluation\n",
    "\n",
    "Separate flow-execution logic from algorithmic logic and **memoize** to speed up computations. That is, cache the results of expensive function calls for later use.\n",
    "\n",
    "- Parallel Computing\n",
    "\n",
    "Simple and easy to debug.\n",
    "\n",
    "- Fast Storage / Compression\n",
    "\n",
    "Better than `pickle` for large objects.\n",
    "\n",
    "(Source: https://hevodata.com/learn/python-batch-processing/.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033fd3a",
   "metadata": {},
   "source": [
    "## `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec136361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▏                                                   | 584478/10000000 [00:00<00:03, 3036507.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████▌                                            | 1762833/10000000 [00:00<00:02, 3750532.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████▋                                        | 2534880/10000000 [00:00<00:01, 3742813.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000001000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████▋                                  | 3638471/10000000 [00:01<00:01, 3572114.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500001500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████▌                            | 4743415/10000000 [00:01<00:01, 3661200.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000002000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████▌                        | 5476357/10000000 [00:01<00:01, 3414897.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500002500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████                  | 6670520/10000000 [00:01<00:00, 3797222.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000003000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████▋           | 7898452/10000000 [00:02<00:00, 3987113.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24500003500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████       | 8709940/10000000 [00:02<00:00, 4023286.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000004000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████▋| 9940042/10000000 [00:02<00:00, 4080300.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40500004500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 10000000/10000000 [00:02<00:00, 3756096.72it/s]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for j in tqdm_.tqdm(range(10000000)):\n",
    "    num += j\n",
    "    if not j % 1000000:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae9d4e",
   "metadata": {},
   "source": [
    "## Contrasting Serial and Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead8352",
   "metadata": {},
   "source": [
    "The function below is based on the following mathematical theorem:\n",
    "\n",
    "$\\large\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\frac{1}{9} - ... = lim_{n\\rightarrow\\infty}\\sum^n_{j=0}\\frac{(-1)^j}{2j+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5376bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_function(row, order, payload):\n",
    "    \"\"\"\n",
    "    Simulate process function\n",
    "    \n",
    "    Row and payload are ignored.\n",
    "    \n",
    "    Approximate pi\n",
    "    \"\"\"\n",
    "    k, pi = 1, 0\n",
    "    for i in range(10**order):\n",
    "        if i % 2 == 0: # even\n",
    "            pi += 4 / k\n",
    "        else:  # odd \n",
    "            pi -= 4 / k \n",
    "        k += 2\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a49ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "order=6\n",
    "N = 1_000\n",
    "items = range(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3adcf",
   "metadata": {},
   "source": [
    "### Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8660df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [batch_process_function(row, order, None) for row in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6569e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1415916535897743"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd069b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 558 ms, total: 1min 57s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Serial run\n",
    "result = [batch_process_function(row, order, None) for row in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71629df",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0475f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1000/1000 [00:23<00:00, 42.85it/s]\n"
     ]
    }
   ],
   "source": [
    "result = joblib.Parallel(n_jobs=8)(\n",
    "    joblib.delayed(batch_process_function)\n",
    "    (row, order, None)\n",
    "    for row in tqdm_.tqdm(items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05f1d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1415916535897743"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6fd11c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1000/1000 [00:22<00:00, 45.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 179 ms, total: 1.28 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Parallel using joblib and a progress bar using tqdm\n",
    "result = joblib.Parallel(n_jobs=8)(\n",
    "    joblib.delayed(batch_process_function)\n",
    "    (row, order, None) \n",
    "    for row in tqdm_.tqdm(items)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852f56b",
   "metadata": {},
   "source": [
    "## Things to Be Aware of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa81f66",
   "metadata": {},
   "source": [
    "- Batch Triggers\n",
    "- Scheduling\n",
    "- Exception Alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b7119",
   "metadata": {},
   "source": [
    "## Serialize per Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df7dde",
   "metadata": {},
   "source": [
    "(Source: https://towardsdatascience.com/parallel-batch-processing-in-python-8dcce607d226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ffbea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2200.29it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "n_workers = 8\n",
    "\n",
    "# Create a batch function\n",
    "def proc_batch(batch, order, matrix):\n",
    "    return [\n",
    "        batch_process_function(row, order, matrix)\n",
    "        for row in batch\n",
    "    ]\n",
    "\n",
    "# Divide data in batches\n",
    "batch_size = np.ceil(len(items) / n_workers)\n",
    "batches = [\n",
    "    items[ix:ix+int(batch_size)] for ix in range(0, len(items), int(batch_size))\n",
    "]\n",
    "\n",
    "# divide the work\n",
    "result = joblib.Parallel(n_jobs=n_workers)(\n",
    "    joblib.delayed(proc_batch)\n",
    "    (batch, order, None) \n",
    "    for batch in tqdm.tqdm(batches)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7682825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(\n",
    "    totals: Union[int, List[int]],\n",
    "    queue : Queue,\n",
    ") -> None:\n",
    "    if isinstance(totals, list):\n",
    "        splitted = True\n",
    "        pbars = [\n",
    "            tqdm(\n",
    "                desc=f'Worker {pid + 1}',\n",
    "                total=total,\n",
    "                position=pid,\n",
    "            )\n",
    "            for pid, total in enumerate(totals)\n",
    "        ]\n",
    "    else:\n",
    "        splitted = False\n",
    "        pbars = [\n",
    "            tqdm(total=totals)\n",
    "        ]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            message = queue.get()\n",
    "            if message.startswith('update'):\n",
    "                if splitted:\n",
    "                    pid = int(message[6:])\n",
    "                    pbars[pid].update(1)\n",
    "                else:\n",
    "                    pbars[0].update(1)\n",
    "            elif message == 'done':\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    for pbar in pbars:\n",
    "        pbar.close()\n",
    "\n",
    "        \n",
    "def task_wrapper(pid, function, batch, queue, *args, **kwargs):\n",
    "    result = []\n",
    "    for example in batch:\n",
    "        result.append(function(example, *args, **kwargs))\n",
    "        queue.put(f'update{pid}')\n",
    "    return result\n",
    "\n",
    "        \n",
    "def batch_process(\n",
    "    items: list,\n",
    "    function: Callable,\n",
    "    n_workers: int=8,\n",
    "    sep_progress: bool=False,\n",
    "    *args,\n",
    "    **kwargs,\n",
    "    ) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "    # Divide data in batches\n",
    "    batch_size = ceil(len(items) / n_workers)\n",
    "    batches = [\n",
    "        items[ix:ix+batch_size]\n",
    "        for ix in range(0, len(items), batch_size)\n",
    "    ]\n",
    "\n",
    "    # Check single or multiple progress bars\n",
    "    if sep_progress:\n",
    "        totals = [len(batch) for batch in batches]\n",
    "    else:\n",
    "        totals = len(items)\n",
    "\n",
    "    # Start progress bar in separate thread\n",
    "    manager = Manager()\n",
    "    queue = manager.Queue()\n",
    "    progproc = Thread(target=progress_bar, args=(totals, queue))\n",
    "    progproc.start()\n",
    "\n",
    "    # Parallel process the batches\n",
    "    result = Parallel(n_jobs=n_workers)(\n",
    "        delayed(task_wrapper)\n",
    "        (pid, function, batch, queue, *args, **kwargs)\n",
    "        for pid, batch in enumerate(batches)\n",
    "    )\n",
    "\n",
    "    # Stop the progress bar thread\n",
    "    queue.put('done')\n",
    "    progproc.join()\n",
    "\n",
    "    # Flatten result\n",
    "    flattened = [item for sublist in result for item in sublist]\n",
    "\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a971edf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m batch_process(items,\n\u001b[1;32m      2\u001b[0m                        batch_process_function,\n\u001b[1;32m      3\u001b[0m                        order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      4\u001b[0m                        n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m                        payload\u001b[38;5;241m=\u001b[39m\u001b[43mmatrix\u001b[49m,\n\u001b[1;32m      6\u001b[0m                        sep_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                       )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matrix' is not defined"
     ]
    }
   ],
   "source": [
    "result = batch_process(items,\n",
    "                       batch_process_function,\n",
    "                       order=6,\n",
    "                       n_workers=8,\n",
    "                       payload=matrix,\n",
    "                       sep_progress=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d17c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joblib",
   "language": "python",
   "name": "joblib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
